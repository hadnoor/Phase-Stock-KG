{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = [\"nasdaq100\", \"sp500\", \"nifty500\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os, sys\n",
    "import pickle\n",
    "import math\n",
    "from queue import PriorityQueue\n",
    "from tkinter import N\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import pickle\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "from sklearn.metrics import accuracy_score, ndcg_score\n",
    "#from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from utils import (mean_absolute_percentage_error,\n",
    "                   load_or_create_dataset_graph,\n",
    "                   mean_square_error, root_mean_square_error)\n",
    "\n",
    "from models.model_sai import Trans\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "#import tensorflow_ranking as tfr\n",
    "\n",
    "# from pytorchltr.loss import LambdaNDCGLoss1, LambdaNDCGLoss2\n",
    "from torchmetrics.functional import retrieval_normalized_dcg\n",
    "\n",
    "from random import randint\n",
    "import wandb\n",
    "GPU = 3\n",
    "LR = 0.0006\n",
    "BS = 128\n",
    "W = 20\n",
    "T = 20\n",
    "LOG = False\n",
    "D_MODEL = 20\n",
    "N_HEAD  = 5\n",
    "DROPOUT = 0.1\n",
    "D_FF    = 1024\n",
    "ENC_LAYERS = 1\n",
    "DEC_LAYERS = 1\n",
    "MAX_EPOCH = 10\n",
    "USE_POS_ENCODING = False\n",
    "USE_GRAPH = [True, False, 'hgat', 'rgat', 'gcn'][2]\n",
    "HYPER_GRAPH = [True, False][1]\n",
    "USE_RELATION_GRAPH = ['gcn', 'hypergraph', 'with_sector', False][3]\n",
    "USE_KG = [True, False][1]\n",
    "PREDICTION_PROBLEM = 'value'\n",
    "RUN = randint(1, 100000)\n",
    "PLOT = False\n",
    "MODEL_TYPE = '' #'random'\n",
    "ENCODER_LAYER = ['gru', 'transf', 'lstm'][2]\n",
    "\n",
    "tau_choices = [5]\n",
    "tau_positions = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_or_create_dataset_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m PREDICTION_PROBLEM \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/pickle/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnasdaq100\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/full_graph_data_correct-P25-W\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-T\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(PREDICTION_PROBLEM)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m dataset, company_to_id, graph, hyper_data \u001b[38;5;241m=\u001b[39m load_or_create_dataset_graph(INDEX\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnasdaq100\u001b[39m\u001b[38;5;124m\"\u001b[39m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, save_path\u001b[38;5;241m=\u001b[39msave_path, problem\u001b[38;5;241m=\u001b[39mPREDICTION_PROBLEM, fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_or_create_dataset_graph' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "PREDICTION_PROBLEM = 'value'\n",
    "save_path = \"data/pickle/\"+INDEX+\"/full_graph_data_correct-P25-W\"+str(20)+\"-T\"+str(20)+\"_\"+str(PREDICTION_PROBLEM)+\".pkl\"\n",
    "\n",
    "dataset, company_to_id, graph, hyper_data = load_or_create_dataset_graph(INDEX=\"nasdaq100\", W=20, T=20, save_path=save_path, problem=PREDICTION_PROBLEM, fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_choices = [1,5,20]\n",
    "tau_positions = [1, 5, 20]\n",
    "\n",
    "\n",
    "def collate_fn(instn):\n",
    "    tkg = instn[0][1]\n",
    "    instn = instn[0][0]\n",
    "    print(\"hellop\",file=sys.stderr)\n",
    "    # df: shape: Companies x W+1 x 5 (5 is the number of features)\n",
    "    df = torch.Tensor(np.array([x[0] for x in instn])).unsqueeze(dim=2)\n",
    "    #df = torch.Tensor(np.array([x[1] for x in instn])).unsqueeze(dim=2) - torch.Tensor(np.array([x[2] for x in instn])).unsqueeze(dim=2)\n",
    "    for i in range(1, 5):\n",
    "        df1 = torch.Tensor(np.array([x[i] for x in instn])).unsqueeze(dim=2)\n",
    "        df = torch.cat((df, df1), dim=2)\n",
    "    min_val = df.min()\n",
    "    max_val = df.max()\n",
    "\n",
    "    # Normalize tensor to the range [-1, 1]\n",
    "    normalized_tensor = 2 * (df - min_val) / (max_val - min_val) - 1\n",
    "    # Shape: Companies x 1\n",
    "    target = torch.Tensor(np.array([x[7][tau_pos] for x in instn]))\n",
    "\n",
    "    # Shape: Companies x 1\n",
    "    #movement = target >= 1\n",
    "\n",
    "    best_case, worst_case = torch.Tensor(np.array([x[11][tau_pos+1] for x in instn])), torch.Tensor(np.array([x[10][tau_pos+1] for x in instn]))\n",
    "    best_case = best_case / torch.Tensor(np.array([x[10][0] for x in instn]))\n",
    "    worst_case = worst_case / torch.Tensor(np.array([x[11][0] for x in instn]))\n",
    "\n",
    "    return (normalized_tensor, target, tkg, best_case, worst_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# for tau in tau_choices:\n",
    "#     tau_pos = tau_positions.index(tau)\n",
    "#     start_time, train_begin = 0, 0\n",
    "    \n",
    "#     def collate_fn(instn):\n",
    "#         tkg = instn[0][1]\n",
    "#         instn = instn[0][0]\n",
    "#         print(\"hellop\",file=sys.stderr)\n",
    "#         # df: shape: Companies x W+1 x 5 (5 is the number of features)\n",
    "#         df = torch.Tensor(np.array([x[0] for x in instn])).unsqueeze(dim=2)\n",
    "#         #df = torch.Tensor(np.array([x[1] for x in instn])).unsqueeze(dim=2) - torch.Tensor(np.array([x[2] for x in instn])).unsqueeze(dim=2)\n",
    "#         for i in range(1, 5):\n",
    "#             df1 = torch.Tensor(np.array([x[i] for x in instn])).unsqueeze(dim=2)\n",
    "#             df = torch.cat((df, df1), dim=2)\n",
    "#         min_val = df.min()\n",
    "#         max_val = df.max()\n",
    "\n",
    "#         # Normalize tensor to the range [-1, 1]\n",
    "#         normalized_tensor = 2 * (df - min_val) / (max_val - min_val) - 1\n",
    "#         # Shape: Companies x 1\n",
    "#         target = torch.Tensor(np.array([x[7][tau_pos] for x in instn]))\n",
    "\n",
    "#         # Shape: Companies x 1\n",
    "#         #movement = target >= 1\n",
    "\n",
    "#         best_case, worst_case = torch.Tensor(np.array([x[11][tau_pos+1] for x in instn])), torch.Tensor(np.array([x[10][tau_pos+1] for x in instn]))\n",
    "#         best_case = best_case / torch.Tensor(np.array([x[10][0] for x in instn]))\n",
    "#         worst_case = worst_case / torch.Tensor(np.array([x[11][0] for x in instn]))\n",
    "\n",
    "#         return (normalized_tensor, target, tkg, best_case, worst_case)\n",
    "#     for phase in range(1, 25):\n",
    "#         train_loader    = DataLoader(dataset[train_begin:start_time+400], 1, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "#         val_loader      = DataLoader(dataset[start_time+400:start_time+450], 1, shuffle=False, collate_fn=collate_fn)\n",
    "#         test_loader     = DataLoader(dataset[start_time+450:start_time+550], 1, shuffle=False, collate_fn=collate_fn)\n",
    "#         print(\"Phase: \", phase)\n",
    "#         with open(\"pickle/Nasdaq_loaders/train_loader/\"+INDEX+str(tau)+\"_\"+str(phase)+\"_25phase.pkl\", \"wb\") as f:\n",
    "#             pickle.dump(train_loader,f) \n",
    "#         with open(\"pickle/Nasdaq_loaders\"+\"/val_loader/\"+INDEX+str(tau)+\"_\"+str(phase)+\"_25phase.pkl\", \"wb\") as f:\n",
    "#             pickle.dump(val_loader,f)\n",
    "#         with open(\"pickle/Nasdaq_loaders\"+\"/test_loader/\"+INDEX+str(tau)+\"_\"+str(phase)+\"_25phase.pkl\", \"wb\") as f:\n",
    "#             pickle.dump(test_loader,f)\n",
    "        \n",
    "#     #print(len(dataset), len(dataset[start_time:start_time+1000]), len(dataset[start_time+1000:start_time+1100]), len(dataset[start_time+1100:start_time+1400]))\n",
    "#         start_time += 100\n",
    "#         if start_time >= 300:\n",
    "#             train_begin += 100   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, train_begin = 0, 0\n",
    "train_loader    = DataLoader(dataset[train_begin:start_time+400], 1, shuffle=True, collate_fn=collate_fn, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase:  1\n",
      "Phase:  2\n",
      "Phase:  3\n",
      "Phase:  4\n",
      "Phase:  5\n",
      "Phase:  6\n",
      "Phase:  7\n",
      "Phase:  8\n",
      "Phase:  9\n",
      "Phase:  10\n",
      "Phase:  11\n",
      "Phase:  12\n",
      "Phase:  13\n",
      "Phase:  14\n",
      "Phase:  15\n",
      "Phase:  16\n",
      "Phase:  17\n",
      "Phase:  18\n",
      "Phase:  19\n",
      "Phase:  20\n",
      "Phase:  21\n",
      "Phase:  22\n",
      "Phase:  23\n",
      "Phase:  24\n",
      "Phase:  1\n",
      "Phase:  2\n",
      "Phase:  3\n",
      "Phase:  4\n",
      "Phase:  5\n",
      "Phase:  6\n",
      "Phase:  7\n",
      "Phase:  8\n",
      "Phase:  9\n",
      "Phase:  10\n",
      "Phase:  11\n",
      "Phase:  12\n",
      "Phase:  13\n",
      "Phase:  14\n",
      "Phase:  15\n",
      "Phase:  16\n",
      "Phase:  17\n",
      "Phase:  18\n",
      "Phase:  19\n",
      "Phase:  20\n",
      "Phase:  21\n",
      "Phase:  22\n",
      "Phase:  23\n",
      "Phase:  24\n",
      "Phase:  1\n",
      "Phase:  2\n",
      "Phase:  3\n",
      "Phase:  4\n",
      "Phase:  5\n",
      "Phase:  6\n",
      "Phase:  7\n",
      "Phase:  8\n",
      "Phase:  9\n",
      "Phase:  10\n",
      "Phase:  11\n",
      "Phase:  12\n",
      "Phase:  13\n",
      "Phase:  14\n",
      "Phase:  15\n",
      "Phase:  16\n",
      "Phase:  17\n",
      "Phase:  18\n",
      "Phase:  19\n",
      "Phase:  20\n",
      "Phase:  21\n",
      "Phase:  22\n",
      "Phase:  23\n",
      "Phase:  24\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "for tau in tau_choices:\n",
    "    tau_pos = tau_positions.index(tau)\n",
    "    start_time, train_begin = 0, 0\n",
    "    \n",
    "    def collate_fn(instn):\n",
    "        tkg = instn[0][1]\n",
    "        instn = instn[0][0]\n",
    "        print(\"hellop\",file=sys.stderr)\n",
    "        # df: shape: Companies x W+1 x 5 (5 is the number of features)\n",
    "        df = torch.Tensor(np.array([x[0] for x in instn])).unsqueeze(dim=2)\n",
    "        #df = torch.Tensor(np.array([x[1] for x in instn])).unsqueeze(dim=2) - torch.Tensor(np.array([x[2] for x in instn])).unsqueeze(dim=2)\n",
    "        for i in range(1, 5):\n",
    "            df1 = torch.Tensor(np.array([x[i] for x in instn])).unsqueeze(dim=2)\n",
    "            df = torch.cat((df, df1), dim=2)\n",
    "        min_val = df.min()\n",
    "        max_val = df.max()\n",
    "\n",
    "        # Normalize tensor to the range [-1, 1]\n",
    "        normalized_tensor = 2 * (df - min_val) / (max_val - min_val) - 1\n",
    "        # Shape: Companies x 1\n",
    "        target = torch.Tensor(np.array([x[7][tau_pos] for x in instn]))\n",
    "\n",
    "        # Shape: Companies x 1\n",
    "        #movement = target >= 1\n",
    "\n",
    "        best_case, worst_case = torch.Tensor(np.array([x[11][tau_pos+1] for x in instn])), torch.Tensor(np.array([x[10][tau_pos+1] for x in instn]))\n",
    "        best_case = best_case / torch.Tensor(np.array([x[10][0] for x in instn]))\n",
    "        worst_case = worst_case / torch.Tensor(np.array([x[11][0] for x in instn]))\n",
    "\n",
    "        return (normalized_tensor, target, tkg, best_case, worst_case)\n",
    "    for phase in range(1, 25):\n",
    "        train_loader    = DataLoader(dataset[train_begin:start_time+400], 1, shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "        val_loader      = DataLoader(dataset[start_time+400:start_time+450], 1, shuffle=False, collate_fn=collate_fn)\n",
    "        test_loader     = DataLoader(dataset[start_time+450:start_time+550], 1, shuffle=False, collate_fn=collate_fn)\n",
    "        print(\"Phase: \", phase)\n",
    "        with open(\"pickle/Nasdaq_loaders/train_loader/\"+INDEX+str(tau)+\"_\"+str(phase)+\"_25phase.pkl\", \"wb\") as f:\n",
    "            pickle.dump(train_loader,f) \n",
    "        with open(\"pickle/Nasdaq_loaders\"+\"/val_loader/\"+INDEX+str(tau)+\"_\"+str(phase)+\"_25phase.pkl\", \"wb\") as f:\n",
    "            pickle.dump(val_loader,f)\n",
    "        with open(\"pickle/Nasdaq_loaders\"+\"/test_loader/\"+INDEX+str(tau)+\"_\"+str(phase)+\"_25phase.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_loader,f)\n",
    "        \n",
    "    #print(len(dataset), len(dataset[start_time:start_time+1000]), len(dataset[start_time+1000:start_time+1100]), len(dataset[start_time+1100:start_time+1400]))\n",
    "        start_time += 100\n",
    "        if start_time >= 300:\n",
    "            train_begin += 100   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "tau_choices = [1,5,20]\n",
    "tau_positions = [1, 5, 20]\n",
    "for tau in tau_choices:\n",
    "    tau_pos = tau_positions.index(tau)\n",
    "    # print(tau_pos)\n",
    "    print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'collate_fn' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickle/Nasdaq_loaders/train_loader/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mINDEX\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(tau)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_25phase.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'collate_fn' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"pickle/Nasdaq_loaders/train_loader/\"+INDEX+str(tau)+\"_\"+str(1)+\"_25phase.pkl\", \"rb\") as f:\n",
    "    train_loader = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
